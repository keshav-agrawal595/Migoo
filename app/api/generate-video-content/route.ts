import { db } from "@/config/db";
import { groq } from "@/config/groq";
import { chapterContentSlides } from "@/config/schema";
import { GENERATE_VIDEO_PROMPT } from "@/data/Prompt";
import { put } from "@vercel/blob";
import { eq } from "drizzle-orm";
import { NextRequest, NextResponse } from "next/server";

// ElevenLabs Speech-to-Text Helper Functions
async function submitAudioForTranscription(audioUrl: string): Promise<string> {
    console.log('üì§ Submitting audio URL to ElevenLabs for transcription:', audioUrl);

    const formData = new FormData();
    formData.append('model_id', 'scribe_v2');
    formData.append('cloud_storage_url', audioUrl);
    formData.append('language_code', 'en');
    formData.append('split_on_words', 'true');

    const response = await fetch('https://api.elevenlabs.io/v1/speech-to-text', {
        method: 'POST',
        headers: {
            'xi-api-key': process.env.ELEVENLABS_API_KEY!
        },
        body: formData
    });

    if (!response.ok) {
        const errorText = await response.text();
        console.error(`‚ùå ElevenLabs transcription submission failed (${response.status}):`, errorText);
        throw new Error(`ElevenLabs API error: ${response.status} - ${errorText}`);
    }

    const result = await response.json();
    console.log('‚úÖ Transcription submitted successfully. ID:', result.transcription_id);

    return result.transcription_id;
}

async function getTranscriptionResult(transcriptionId: string, maxRetries: number = 30): Promise<any> {
    console.log('üîÑ Polling for transcription result:', transcriptionId);

    for (let i = 0; i < maxRetries; i++) {
        const response = await fetch(
            `https://api.elevenlabs.io/v1/speech-to-text/transcripts/${transcriptionId}`,
            {
                headers: {
                    'xi-api-key': process.env.ELEVENLABS_API_KEY!
                }
            }
        );

        if (!response.ok) {
            const errorText = await response.text();
            console.error(`‚ùå Failed to get transcription result (${response.status}):`, errorText);
            throw new Error(`Failed to retrieve transcription: ${response.status}`);
        }

        const result = await response.json();

        // Check if transcription is complete
        if (result.text && result.words) {
            console.log('‚úÖ Transcription completed successfully');
            return result;
        }

        // Wait before next poll (2 seconds)
        console.log(`‚è≥ Waiting for transcription... Attempt ${i + 1}/${maxRetries}`);
        await new Promise(resolve => setTimeout(resolve, 2000));
    }

    throw new Error('Transcription timed out after maximum retries');
}

// Convert word-level timestamps to sentence chunks
function wordsToChunks(words: any[]): { timestamp: [number, number] }[] {
    if (!words || words.length === 0) return [];

    const chunks: { timestamp: [number, number] }[] = [];
    let currentChunk: any[] = [];
    let chunkStartTime = words[0].start;

    for (let i = 0; i < words.length; i++) {
        const word = words[i];
        currentChunk.push(word);

        // Check if this is end of a sentence or chunk
        // Create chunk on: punctuation, max 10 words, or last word
        const isPunctuation = word.text.match(/[.!?]$/);
        const isMaxLength = currentChunk.length >= 10;
        const isLastWord = i === words.length - 1;

        if (isPunctuation || isMaxLength || isLastWord) {
            const chunkEndTime = word.end;
            chunks.push({
                timestamp: [chunkStartTime, chunkEndTime]
            });

            // Start new chunk
            currentChunk = [];
            if (i < words.length - 1) {
                chunkStartTime = words[i + 1].start;
            }
        }
    }

    return chunks;
}

async function generateCaptions(audioUrl: string): Promise<any> {
    try {
        console.log('üéØ Starting caption generation for audio:', audioUrl);

        // Step 1: Submit audio for transcription
        const transcriptionId = await submitAudioForTranscription(audioUrl);

        // Step 2: Poll for transcription result
        const transcription = await getTranscriptionResult(transcriptionId);

        // Step 3: Convert words to chunks for reveal timing
        const chunks = wordsToChunks(transcription.words);

        // Step 4: Format captions data with chunks
        const captions = {
            text: transcription.text,
            language_code: transcription.language_code,
            chunks: chunks  // This is what the frontend expects
        };

        console.log(`‚úÖ Captions generated: ${chunks.length} chunks from ${transcription.words.length} words`);
        return captions;

    } catch (error: any) {
        console.error('‚ùå Caption generation failed:', error.message);
        throw error;
    }
}

export async function POST(req: NextRequest) {
    try {
        const { chapter, courseId } = await req.json();

        console.log('üé¨ Starting video content generation for:', {
            courseId,
            chapterId: chapter.chapterId,
            chapterTitle: chapter.chapterTitle
        });

        // Check if slides already exist for this chapter
        const existingSlides = await db
            .select()
            .from(chapterContentSlides)
            .where(eq(chapterContentSlides.chapterId, chapter.chapterId));

        if (existingSlides.length > 0) {
            console.log(`‚úÖ Slides already exist for chapter ${chapter.chapterId}. Skipping generation.`);
            console.log(`üìä Found ${existingSlides.length} existing slides`);

            return NextResponse.json({
                success: true,
                data: existingSlides,
                skipped: true,
                message: 'Slides already exist for this chapter',
                metadata: {
                    generatedAt: existingSlides[0].createdAt,
                    courseId,
                    chapterId: chapter.chapterId,
                    totalSlides: existingSlides.length
                }
            });
        }

        // Generate video slides using Groq AI
        const result = await groq.json(
            GENERATE_VIDEO_PROMPT,
            JSON.stringify(chapter),
            {
                model: 'openai/gpt-oss-120b',
                temperature: 0.7,
                max_tokens: 4000
            }
        );

        console.log('‚úÖ Groq API Response Received:', {
            resultType: typeof result,
            isArray: Array.isArray(result),
            length: Array.isArray(result) ? result.length : 'N/A'
        });

        if (!Array.isArray(result)) {
            throw new Error('Groq API did not return an array of slides');
        }

        if (result.length > 0) {
            console.log('üìä First slide preview:', JSON.stringify(result[0], null, 2));
        }

        const VideoContentJson = result;
        const insertedSlides = [];

        // Process each slide: generate audio, upload to Vercel Blob, and save to database
        for (let i = 0; i < VideoContentJson.length; i++) {
            const slide = VideoContentJson[i];
            const narration = slide.narration.fullText;

            console.log(`üé§ Generating audio for slide ${i + 1}/${VideoContentJson.length}: ${slide.slideId}`);
            console.log(`üìù Narration text (${narration.length} chars):`, narration.substring(0, 100) + '...');

            try {
                // Generate audio using Groq Orpheus TTS API
                const response = await fetch("https://api.groq.com/openai/v1/audio/speech", {
                    method: "POST",
                    headers: {
                        "Authorization": `Bearer ${process.env.GROQ_API_KEY}`,
                        "Content-Type": "application/json"
                    },
                    body: JSON.stringify({
                        model: "canopylabs/orpheus-v1-english",
                        voice: "troy",
                        input: narration,
                        response_format: "wav"
                    })
                });

                if (!response.ok) {
                    const errorText = await response.text();
                    console.error(`‚ùå Groq TTS API Error (${response.status}):`, errorText);
                    throw new Error(`Groq TTS failed: ${response.status} - ${errorText}`);
                }

                console.log('‚úÖ Groq TTS Response received');

                // Get audio buffer from response
                const arrayBuffer = await response.arrayBuffer();
                const audioBuffer = Buffer.from(arrayBuffer);
                console.log(`üì¶ Audio buffer size: ${audioBuffer.length} bytes`);

                // Validate audio buffer size
                if (audioBuffer.length < 1000) {
                    console.error('‚ö†Ô∏è Suspiciously small audio buffer!');
                    throw new Error(`Audio buffer too small (${audioBuffer.length} bytes). Expected larger WAV file.`);
                }

                // Upload audio to Vercel Blob
                const filename = `audio/${courseId}/${chapter.chapterId}/${slide.slideId}.wav`;
                const { url } = await put(filename, audioBuffer, {
                    access: 'public',
                    contentType: 'audio/wav',
                    allowOverwrite: true
                });

                console.log(`‚úÖ Audio uploaded to Vercel Blob: ${url}`);

                // Generate captions from audio using ElevenLabs (with chunks)
                console.log('üé¨ Generating captions from audio...');
                const captions = await generateCaptions(url);

                // Insert slide data into database with captions
                const [insertedSlide] = await db.insert(chapterContentSlides).values({
                    courseId: courseId,
                    chapterId: chapter.chapterId,
                    slideId: slide.slideId,
                    slideIndex: slide.slideIndex,
                    audioUrl: url,
                    narration: slide.narration,
                    captions: captions,  // Now contains chunks instead of words
                    html: slide.html,
                    revealData: slide.revealData
                }).returning();

                insertedSlides.push(insertedSlide);
                console.log(`üíæ Slide ${slide.slideId} saved to database with audio URL and captions`);

            } catch (error: any) {
                console.error(`‚ùå Error processing slide ${slide.slideId}:`, error.message);
                throw new Error(`Failed to process slide ${slide.slideId}: ${error.message}`);
            }
        }

        console.log('üéâ Video content generation completed successfully!');

        return NextResponse.json({
            success: true,
            data: insertedSlides,
            metadata: {
                generatedAt: new Date().toISOString(),
                model: 'openai/gpt-oss-120b',
                courseId,
                chapterId: chapter.chapterId,
                totalSlides: insertedSlides.length
            }
        });

    } catch (error: any) {
        console.error('üî• Video content generation failed:', error);
        return NextResponse.json(
            {
                success: false,
                error: error.message || 'Failed to generate video content',
                details: error.stack
            },
            { status: 500 }
        );
    }
}